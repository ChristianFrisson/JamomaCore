% -----------------------------------------------
% Template for SMC 2009
%     smc2009.sty -> style file
% Last modified by Fabien Gouyon (smc2009@inescporto.pt)
% Modified by Juan P. Bello (ismir2008-papers@ismir.net)
% By Rainer Typke (ismir07.rainer@safersignup.com)
% Based on the 2004 template by Eloi Batlle.
% -----------------------------------------------

\documentclass{article}
\usepackage{smc2009,amsmath}
% To use when using pdflatex
\usepackage{graphicx}
\usepackage{url}     
\usepackage{hyperref}   
      
\newenvironment{packed_item}{
\begin{itemize}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{itemize}}

\newenvironment{packed_enumerate}{
\begin{enumerate}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{enumerate}}
% To use when using latex, dvips and ps2pdf
% \usepackage[dvips]{graphicx}

% Title.
% ------
\title{a layered approach to sound spatialization - concepts and examples}

% IMPORTANT NOTICE:
% Reviews are double-blind
% Authors will not be informed of who reviews their papers, and author names will be concealed from the reviewers 
% Please avoid evident self references in the text

% Authors' names must be omitted from title page (or listed as “name(s) omitted for submission”)


% Single address
% To use with only one author or several with the same address
% ---------------
%\oneauthor
%   {Author} {School \\ Email}

% Two addresses
 %--------------
\twoauthors
  {First author} {School \\ Email}
  {Second author} {Company \\ Email}

% Three addresses
% --------------
%\threeauthors
%  {First author} {School \\ Email}
%  {Second author} {Company \\ Email}
%  {Third author} {Company \\ Email}


\begin{document}
%
\maketitle
%

\permission

\begin{abstract} 
-flexible working environment for spatialization\\
-A layered approach towards interoperability in sound spatialization management\\
The abstract should be placed at the top left column and should contain
about 150-200 words.

KEYWORDS : sound spatialization, interoperability, layered architecture, adaptation, site-specific, Ambisonics

\end{abstract}

\section{Introduction}\label{sec:introduction}

 
The goal of each spatialization
%rendering is it only the rendering system, or control/rendering ?
system is, simply speaking, to facilitate or even empower the creative use of spatial aspects for the artist. However, quantitative studies on spatial music (\cite{otondo2008ctu}, \cite{PetersSurvey}) remind us that there are great individual and context-related differences in defining that goal across composers. %that the compositional use of spatialization techniques varies strongly across composers and musical context.    
For instance, the requirements of a computer aided spatialization system may vary between a fixed-media composition (e.g \cite{BarrettOS02} %TODO: find a Robert Normandeau reference
), an art-installation (\cite{lossius:2007sound_space_body}), and a live diffusion performance (\cite{Truax99}). Moreover, different components are in play in the different stages of the creative processes 
\emph{Experimentation -- Composing -- Performance -- Documentation}, for instance between :
\begin{packed_item} 
	\item {real-time or non real-time rendering}  
%	\item {Plug-In structure: exchangeable renderer and interface components} - this is one of the ideas of the article, we don't need it here
	\item {multichannel playback and recording possibility} 	
	\item {binaural rendering for headphone listening}
	\item {testing technical setup, e.g. loudspeaker connections}   
	\item {customizable renderer in order to accommodate for different acoustical conditions, e.g. adapting the virtual room description to the listening room} 
\item {customizable to accommodate for different technical conditions, e.g. rendering to different reproduction formats, compensation for non-ideal loudspeaker configurations, routing signals to dedicated physical outputs}  
	\item {visual representation of a sound scene} 
%\item {separate render and control layer} - this is one of the ideas of the article, we don't need it here
\item {allowing for external control e.g. through MIDI, OSC or other protocols}

\end{packed_item}
TODO: this list must become better!-please help - \emph{what do we want to say with this list ??} \\

%The outlined requirements may differ according to artistic paradigms.
%-Use cases (users : composers, installation artists, scientists...etc...)
%   - OpenMusic off-line rendering
%   - BEAST sound diffusion (the more speaker - the mre complicated to perform)
%   - interactive sound installations
%   - real-time control via sensors/gestural controller etc. by performer
%   - tape-music (fixed media)
%   - computer generated - real time spatialization (see <meta-description> layer)
   
    
%Experimental stage:\\
%\begin{packed_item}
% \item {Plug-In structure: exchangeable renderer and interface components} 
% \item {multichannel recording possibility for capturing of sketches/ideas}         
% \item {present management}
% \item {sound scene visualization - especially for off-line rendering of spatial processes}
%\end{packed_item} 

%Compositional stage:\\  
%\begin{packed_item}
%  \item {binaural rendering for headphone listening} 
%\end{packed_item} 
%Performance stage:\\
%\begin{packed_item} 
%	\item {customizable to accommodate for different technical conditions, e.g. rendering to different reproduction formats, compensation for non-ideal %loudspeaker configurations, routing signals to dedicated physical outputs}   
%    \item {testing technical setup, e.g. loudspeaker connections}
%    \item {customizable to accommodate for different acoustical conditions, e.g. adapting the virtual room description to the listening room} 
%\end{packed_item} 
%Documentation stage:\\  
%\begin{packed_item}
%\item {multichannel recording possibility}         
%\item {storing }       
%\end{packed_item}  


%\quote{ What WE need, for our personal work, is a way to extend the capabilities of those tools in a completely flexible and configurable way - and that suggests plug-ins (though it will always be a potential problem overcoming inherent IO structures in the host applications).}

%\quote{working with non-standard loudspeakers: Meyer Sound Spherical Loudspeaker Array that is under research at CNMAT, or the Hemisphere Point-source Emanation Loudspeaker, for example.  I would like to experiment with these;}

%\quote{Tool building and music making happen together and depend upon each other.}

%\quote{Very frustrated with my current spatialization software and am desperately looking for something better!}

\section{Review}
This section gives an overview of current paradigms and solutions for sound spatialization, and shows their limitations. 


\subsection{digital audio workstations - DAW} \label{sec:digital audio workstations - DAW}  

Currently, many composers and sound-artists use DAWs for designing their sound spatialization, especially for fixed media, tape-music and consumer media production. In these contexts the DAWs generally fullfill the artistic needs of the users in terms of spatialization, along with the fact that they are used to the concept and restrictions of DAWs, and that these offer a good workflow and project management, and are extendable through plug-ins\\

From 1993, emergence of the ITU 5.1 has been announced as a major improvement for mixing surround sound. Originally designed to fulfill the needs of the film industry, this loudspeaker configuration appeared to music and entertainment companies as a new way to extend their commercial offer. Since then, all DAWs surround panners have been designed to respect and organize sound processing for this set-up. We had to wait until the late nineties to see new suggestions for the surround workstations (from 6.1 to 10.2,  \footnote{a comparative table of DAWs concerning multi-channel audio can be found at \url{http://acousmodules.free.fr/hosts.htm}}), enhancing sides precision and perception outside of the Central Listening Position.\\
This was an useful approach for recording, multimedia or film prospects (cannot do business without standards), but still in a regular, circular, geometric and (FIXME: non-neutral? we mean it influences the user) design, wich cannot be acceptable and sufficient for creation or live applications. As an example, a non-flexible loudspeaker circle set-up is very rarely adapted to the concert hall.\\ 
As much as the speaker configuration, another critical fonction for surround sound mixing is the way in which the energy is parsed between the speakers. A ``blur" parameter (often confused with the ``divergence" parametrer control : \quote{which determines the width of the panned signal with respect to neighboring speakers}\cite{digidesignSPG2005})  should be one of the most adjustable, and could appear on every surround panner affected to audio channels. Because it enables many variations and possibilities for a creative mix, and because we would work with this parameter considering the specific features of the sound we have to spatialize. Most of the time, and too restrictively, this factor is associated and linked to the sound source's position to the center in the surround panner, and is related to parameters such as ``spread" or ``diversity" (e.g. in Apple's Logic Pro \footnote{\url{www.apple.com/logicstudio/}}). The most accurate and efficient representation for this parameter is available on Sequoia software \footnote{\url{http://www.magix.com/us/sequoia/}}, with ``soundfield'' offset and character (shape) settings for each track, a global divergence to fit to the listening room size, and a complete flexibility for the speaker set-up in the listening area. \\

%
% Did we discuss all point below:? (moved above -> we should use them at the beginning IMHO)
%
%
%spatialization is mainly done in DAWs and leads to "panning" because the bus architectures and interfaces of the panning plugins suggests this.
%    Limitations:
%        a) mainly tied to consumer formats (stereo and ITU 5:1 surround)
%        b) restricted to linear prerendering compositional processes
%        c) complicated to maintain automation when changing rendering plugIn
%        d) burden to connect interfaces (Lemur, Stantum, Wacom, Camera-tracking)
%        
%        
\subsection{Media programming environments}
Beside the paradigm of the DAW, various media programming environment exist, such as SuperCollider, Pure Data, Chuck, OpenMusic, or Max/MSP which are capable for spatial sound synthesis. In order to support individual approaches and to meet the specific needs of computer music and mixed media art, these environments enable the user to combine music making with computer programming.  
However, in the name of complete flexibility, these environments lack in providing structured solutions for the specific challenges in spatial music, as outlined in section \ref{sec:introduction}. Consequently, enumerable different self-contained libraries and spatialization toolboxes were created by artists and researcher, such as the Space Unit Generator \cite{SUG02}, IRCAM's Spatialisateur \cite{JotPhD}, or Virtual Microphone Control \cite{CMJ08-VIMIC} to generate virtual sound sources and artificial spaces. Also frameworks primarily dedicated to sound diffusion practice, such as the BEASTmulch System\footnote{\url{http://www.beast.bham.ac.uk/research}}, or ICAST \cite{ICAST06} are available.   
However, each tool may only provide solutions for a subset of compositional viewpoints and the development of new compositional approaches through combining these tools is difficult or even limited through their specific designs. 
%LIMITATIONS OF THESE SOLUTIONS :

\section{Strategies \& Methodologies in Spatial Sound Synthesis}  
 
A traditional workflow in electroacoustic composition or linear sound editing comprises a number of steps leading to the construction of the audio scene. Other contexts such as audio installations or multimedia work imply a different sequence in the steps of the process. The modular approach tries to abstract these differences and define underlying common elements that are always in play when spatialization is used. This layer model doesn't imply a specific method, it much rather tries to emphasize the importance of the interchangeability of modules, which share their interconnecting interfaces. The tools for each layer can be exchanged and compared, without forcing the artist to completly rebuild his setup.

\begin{quote}Spatial orchestration eases this [...] constraint, [...] the composition may be reconfigured for each individual performance. This will require [...] the composer [to] conceive spatial attributes in a more abstract fashion, that is then instantiated in potentially different ways into different performance environments. \cite{Lyon:2008spatial_orchestration} \end{quote}
By working symbolically during the composition/editing process and separating the authoring from the rendering processes a realization can become portable. A common set of descriptors and shared representation metaphors such as timelines or scene graphs builds the glue to make a disparate set of tools into a coherent workflow by enhancing interoperability between the different building blocks.\\
Modularity is another key point towards this aim, as it allows to separate elements into different layers, according to their function, as describeb in \ref{sec:layers}. It also allows adaptability, through modules querying other modules to adapt themselves to other parts of the graph.\\
The Tools have to comply to these recommandations, and can be separated in two categories : rendering engines and authoring meta-tools.\\


\subsection{A layered approach to the spatialization workflow}\label{sec:layers}
%TROND, J45CH\\
In order to better grasp the structural and methodological entities used in a spatialization process, a layered approach is proposed, which defines the problem domains in an ascending order of abstraction (Table \ref{tab:layer}). This model gets its inspiration from the OSI network model \footnote{\url{http://en.wikipedia.org/wiki/OSI_model}}



%\begin{packed_enumerate}   
%\item[5.]{controlling the rendering (e.g. HoloEdit, ambicontrol, swarms, boids \cite{kimboyle:ssp}, trajectories...)     <indirect control %description or meta-description>
%       Protocol: SpatDIF extended }
%\item[4.]{position of sources and speakers at one moment in time as SpatDIF                          <direct control description>
%       Protocol: SpatDIF core, SpatDIF extended if required by the encoder }
%\item[3.]{encoded audio
%       Protocols:
%           Spatial infomation embedded in format: (e.g. B-format and Dirac, MPEG-surround ?)
%           SpatDIF core and extended if required by the encoder (concerning info on target system) }
%\item[2.]{decoded audio
%       Protocols: E.g. CoreAudio, Jack, PortAudio }
%\item[1.]{physical layer (computers, sound cards, speakers, etc.)}
%\end{packed_enumerate}  
%TODO: sketch of the layers?

\begin{table*}   
	\begin{tabular}{llll} 
	Nr. & Layer & Examples&\\
	\hline
	5 & controlling the rendering & HoloEdit, ambicontrol,& indirect control description \\ 
	  &    &  swarm simulator, algorithmic generator & or meta-description\\       %, boids \cite{kimboyle:ssp}
	\hline
	4. & audio scene description  & Protocol: SpatDIF core protocol,  & current position of virtual sound sources, \\
	   &             &    SpatDIF extended if required  & loudspeakers, source width, ...\\
	\hline
	3. & encoded audio Protocols & B-format, DIRAC, MPEG-surround & Spatial infomation embedded in format\\
 	\hline 
	2. & decoded audio Protocols & CoreAudio, Jack, PortAudio & \\	
	\hline
	1. & physical layer & computers, sound cards, loudspeakers& \\			 
	%\hline
	%Distinctness & ??& ??\\			
   	\end{tabular}               
  			\caption{Layers}
		\label{tab:layer}       
	\end{table*}                          




\subsection{SpatDIF}

The goal of SpatDIF is to develop a system-independent language for describing spatial audio scenes \cite{Peters:2008spatdif} 
Formats who integrate spatial audio descriptors such as MPEG-4 Advanced Audio BIFS or OpenAL did not fully succeed in the music or fine arts community because these formats are primary tailored to multimedia and gaming applications and do not necessarily consider the special requirements of spatial music and performances in concert venues or sonic/mixed media installations in specific places such as galleries or museums. Furthermore it is the authors opinion that artists were not sufficiently involved in the development of these formats.\\
As can be seen in the layer section (... to be continued ...)



\section{Tools}
%- structured according to layers

In order to concretely experiment these ideas, the authors designed and used the tools described below :

\subsection{ICST Ambisonics}
The ICST ambisonics tools comprise a set of four externals for MaxMSP. The two DSP externals ambiencode~ and ambidecode~ generate and decode Higher Order Ambisonics. They reach 3rd order with Furse-Malham Formulas in version 1.2, and 5th order using either the Furse-Malham or Normalized 3D formulas in version 2.0. The two control externals ambimonitor and ambicontrol complete the set. Ambimonitor presents the user with a GUI displaying point sources in an abstract 2D or 3D space, various key commands, snapshot and file I/O capabilities and it generates coordinate information for the DSP objects. Ambicontrol provides a number of methods that control motion of points in the Ambimonitor's dataset. Automated motions such as rotation, random motion, optionally constrained in bounding volumes and user defined trajectories can be applied to single or grouped points. The import/export format for the trajectories and state snapshots is a simple XML text file, which will be replaced by a SpatDIF compliant formatting in a next release. \cite{Schacher:2006ambi_max} At ICST a different panning algorithm was derived from in-phase ambisonic decoding and also coded as MaxMSP external. Ambipanning~ implements the Ambisonic Equivalent Panning, which transcodes in one step a set of mono sources onto a ideally circular sepaker setup with an arbitrary number of speakers. The algorithm works with a continuous order factor, which permits to apply per point individually varying directivity responses. This external understands the same command syntax and becomes interchangeable with the other ICST ambisonic DSP objects. \cite{Neukom:2008ambipan}

\subsection{Jamoma}

Jamoma\footnote{\url{http://www.jamoma.org/}}  is a collaborative set of software tools, including DSP libraries, and a framework for structuring Max/MSP patches.\\
As a part of the Jamoma library, the DBAP algorithm \cite{dbapICMC09} has been developped as a Max/MSP external. Its major interest consists in how it goes beyond the loudspeakers setup limitation explained in \ref{sec:digital audio workstations - DAW}, based on an ideal model rather than actual positions of loudspeakers in the room, which can be particularly usefull for theatre or installation uses.\\
One other important part of the Jamoma modular framework\cite{Place:2006jamoma} supplies a set of spatialization tools, as modules wrapping different spatialization algorithms (VBAP FIXME:\cite{Pulkki:2000vbap_max}, DBAP \cite{dbapICMC09}, ambisonics \cite{Schacher:2006ambi_max}, ViMic\cite{CMJ08-VIMIC}) that can be combined and substituted one to each other in order to, e.g. quickly compare the adequation of each of them to a specific context. 
Moreover, a number of effect processors (Doppler, Airfilter, RollOff, Reverb) and utilities (loudspeaker setup, FIXME:SPL compensation, FIXME:delay compensation, multichannel meters and auxilliaries) are also available in the distribution.
All these modules have been designed according to the same namespace model, thus allowing the user to seamlessly connect the modules  in minutes and thus experiment ideas presented here, in particular, the actual interoperability of different algorithms, Holo-Edit, Ambimonitor GUI object and physical interfaces (i.e. JazzMutant's Lemur \footnote{\url{http://www.jazzmutant.com/}} or TUIO-compliant \cite{kaltenbrunner2005tpt} devices using the Max Multitouch Framework \footnote{\url{http://code.google.com/p/mmf/}}).\\
Moreover, an ongoing effort has be done in developping Jamoma Multicore \footnote{\url{http://code.google.com/p/jamulticore/}}, which is a coding layer for the creation of dynamic audio graph topographies, allowing connections carrying multiple channels of audio.\\
TODO : develop the multicore thingie (Tim)

 
\subsection{Holo-Edit}
Holo-Edit, initiated by L. Pottier \cite{PottierDAFX98}, is part of the GMEM Holophon project, dedicated to multi-sources and multi-speakers spatialization. This standalone application\cite{Pottier05} is written in Java / OpenGL and uses the timeline paradigm found in traditional DAW for recording, editing, and reproducting spatialization control messages. The data is manipulated in the form of trajectories or sequences of time-tagged points in a 3D space. These trajectories can be generated or modified by an expandable set of tools allowing specific spatial and temporal behaviors, including amongst others symmetry, proportion, translation, acceleration, local exaggeration... Different scene representation windows allow the user to view/modify data focusing on different aspects : ``Room" by a top view of the virtual space, ``Time Editor" which is related to the classic automation curve view in DAWs and ``Score" which represent the whole composition in a multi-track block-based view.
An important feature of Holo-Edit is the ability to represent sound waveforms beside their trajectories. This allows to synchronize sound cues to their associated movements.
The model of space/time representation has been left intentionally generic, thus it can control a variety of different spatialization methods. Even if the model used is a ``virtual space" model [ TODO : ref ?? ], its generic nature allows to use it in a more abstract way, thinking more of movements in an arbitrary space ( may it be acoustic, symbolic, semiotic ...).
Holo-Edit uses OSC for I/O data communication. It has been successfully used with various spatial renderers, written in Max/MSP, Pure Data or SuperCollider, using various algorithms like VBAP, Ambisonics, DBAP. 
Though, the main drawback of this workflow is the important process of adapting and formating the data to fit the algorithm inputs (e.g. coordinate system, scales, ). 
A communication interface has then been developped and integrated into the Jamoma environnement. It thus adds the features of playing trajectory compositions according to the edited score and to record them from any real-time control inputs available in Jamoma. 
  


\section{Discussion \& Outlook}

The method used here tries to define standards through experimental implementation and usage :

This collaborative research has emerged from a workshop... we're planning to make more of them

The next step includes developing more usecases and a wider set of tools. SpatDIF and the modular approach to spatialization needs to grow through implementations in a variety of software, especially commercially used systems DAWs and plugins.


\section{Conclusion}
A conclusion is the place when you get tired of thinking. \\

\section{Acknowledgment}
\bibliographystyle{abbrv}  %\bibliographystyle{natbib}% ama, nar, alpha, plain, chicago,{plainnat}  abbrv, siam   
\small
\bibliography{smc09}
\end{document}   

%%%%%%%  Poubelle %%%%%%   
	
%\subsection{Frameworks for Spatialization}
%\textbf{Spatialisateur} (Spat) \cite{JotPhD}, in development at IRCAM and Espaces Nouveaux since 1991, is a library of spatialization algorithms for MaxMSP, including VBAP, first-order Ambisonics and stereo techniques (XY, MS, ORTF) for up to 8 loudspeakers. It can also reproduce 3D sound for headphones (binaural) or 2/4 loudspeakers (transaural). A room model is included to create artificial early reflections and reverb controlled by a perceptual-based user interface.\\
%\textbf{Ambisonics toolbox}  \cite{Schacher:2006ambi_max}

   

% \section{Style Stuff - delme}

% \subsection{Title and Authors}

%The title is 14pt Times, bold, caps, upper case, centered. Authors' names are centered. 
%The lead author's name is to be listed first (left-most), and the co-authors' names after. If the addresses for all 
%authors are the same, include the address only once, centered. If the authors have different addresses, 
%put the addresses, evenly spaced, under each authors' name.

% Reviews are double-blind. {\it \textbf{Author information should be removed from this page in initial submissions}}, 
% and only added later by the authors when sending camera-ready versions.


%\subsection{Figures, Tables and Captions}

%All artwork must be centered, neat, clean, and legible. All lines should
%be very dark for purposes of reproduction and art work should not be hand-drawn.
%The proceedings is not in color, and therefore all figures must make
%sense in black-and-white form.
%Figure and table numbers and captions always appear below the figure. Leave 1
%line space between the figure or table and the caption. Each figure or table
%is numbered consecutively. Captions should be Times 10pt.
%Place tables/figures in text as close to the reference as possible.
%References to figures and tables should be capitalised, for example:
%see Figure \ref{fig:example} and Table \ref{tab:example}.
%Figures and tables may extend across both columns to a maximum
%width of 7'' (17.78~cm).

%\begin{table}
%\begin{center}
%\begin{tabular}{|l|l|}
%\hline
%String value & Numeric value \\
%\hline
%hello SMC  & 1073 \\
%\hline
%\end{tabular}
%\end{center}
%\caption{Table captions should be placed below the table}
%\label{tab:example}
%\end{table}

%\begin{figure}
%\centerline{\framebox{
% To use when using pdflatex
%	\includegraphics[width=\columnwidth]{../ICMC2009-dbap/all_r_6_b_0_2}}}
	% To use when using latex, dvips and ps2pdf
% 	\includegraphics[width=\columnwidth]{figure.eps}}}
%\caption{Figure captions should be placed below the figure}
%\label{fig:example}
%\end{figure}




